{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d72558",
   "metadata": {},
   "source": [
    "# Anime Recommender - Collaborative filtering\n",
    "\n",
    "This project aims to build an anime recommender using collaborative filtering, \n",
    "a method that predicts user preferences by anticipating what someone with similar tastes would also enjoy. \n",
    "\n",
    "Collaborative filtering comes in two forms:\n",
    "\n",
    "- **User-based:** Recommends items by finding similar users and suggesting items that they have liked or interacted with.\n",
    "- **Item-based:** Recommends items by finding similar items to those that the user has shown interest in. Note that this is different from content-based filtering as the 'similarity' is based on its relationship with users, not the content.\n",
    "\n",
    "By leveraging user interactions and item similarities, this recommender provides personalized anime recommendations based on user preferences and behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789678b",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47631bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a843cf1",
   "metadata": {},
   "source": [
    "## Import cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "611c37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anime_reviews = pd.read_csv(\"datasets/anime_review_cleaned.csv\")\n",
    "anime_data = pd.read_csv(\"datasets/anime_2020_clean.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c180bd3",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56723c36",
   "metadata": {},
   "source": [
    "### Preparation: Merging the dataset\n",
    "We would filter create the user-item matrix as both user-based and item-based uses the same matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d11f76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## REMOVE LATER MAYBE-------\n",
    "# anime_data = anime_data.drop('score', axis=1)\n",
    "##THIS LINE -------\n",
    "\n",
    "# Merge data\n",
    "merged_data = pd.merge(anime_data, reviews_data, left_on='uid', right_on='anime_uid')\n",
    "# merged_data.head()\n",
    "\n",
    "# Define rating scale\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "\n",
    "# Load data into Surprise dataset format\n",
    "data = Dataset.load_from_df(merged_data[['profile', 'uid', 'score']], reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6148dbd",
   "metadata": {},
   "source": [
    "## Attempt 1: User-based collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf15855",
   "metadata": {},
   "source": [
    "### Create and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ea51cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x16c9e9c90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build user-based collaborative filtering model\n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "model = KNNBasic(sim_options=sim_options)\n",
    "model.fit(trainset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dabe51",
   "metadata": {},
   "source": [
    "### Checking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c577aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.9756\n",
      "RMSE: 1.9756028737805376\n"
     ]
    }
   ],
   "source": [
    "# from surprise import accuracy\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = rmse(predictions)\n",
    "print(\"RMSE:\", accuracy)\n",
    "\n",
    "\n",
    "# # Calculate RMSE and MAE\n",
    "# rmse = accuracy.rmse(predictions)\n",
    "# mae = accuracy.mae(predictions)\n",
    "\n",
    "# print(\"RMSE:\", rmse)\n",
    "# print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7db201",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ace80a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test for a user in the dataset\n",
    "\n",
    "# Generate recommendations for a specific user\n",
    "user_id = 'skrn'\n",
    "anime_ids = [uid for uid in anime_data['uid'].values if uid not in merged_data[merged_data['profile'] == user_id]['uid'].values]\n",
    "\n",
    "# Predict ratings for items not rated by the user\n",
    "predicted_ratings = {}\n",
    "for anime_id in anime_ids:\n",
    "    predicted_rating = model.predict(user_id, anime_id).est\n",
    "    predicted_ratings[anime_id] = predicted_rating\n",
    "\n",
    "# Sort predicted ratings and recommend top N items\n",
    "top_n = 10\n",
    "recommended_anime_ids = sorted(predicted_ratings, key=predicted_ratings.get, reverse=True)[:top_n]\n",
    "\n",
    "# Print recommended anime titles\n",
    "recommended_anime_titles = anime_data[anime_data['uid'].isin(recommended_anime_ids)]['title'].values\n",
    "print(\"Recommended Anime Titles:\")\n",
    "for title in recommended_anime_titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16e96fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "      <th>aired</th>\n",
       "      <th>episodes</th>\n",
       "      <th>members</th>\n",
       "      <th>popularity</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>10165</td>\n",
       "      <td>Nichijou</td>\n",
       "      <td>Nichijou  primarily focuses on the daily antic...</td>\n",
       "      <td>['Slice of Life', 'Comedy', 'School', 'Shounen']</td>\n",
       "      <td>Apr 3, 2011 to Sep 25, 2011</td>\n",
       "      <td>26.0</td>\n",
       "      <td>497276</td>\n",
       "      <td>137</td>\n",
       "      <td>https://myanimelist.net/anime/10165/Nichijou</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid     title                                           synopsis  \\\n",
       "4211  10165  Nichijou  Nichijou  primarily focuses on the daily antic...   \n",
       "\n",
       "                                                 genre  \\\n",
       "4211  ['Slice of Life', 'Comedy', 'School', 'Shounen']   \n",
       "\n",
       "                            aired  episodes  members  popularity  \\\n",
       "4211  Apr 3, 2011 to Sep 25, 2011      26.0   497276         137   \n",
       "\n",
       "                                              link  \n",
       "4211  https://myanimelist.net/anime/10165/Nichijou  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "anime_data[anime_data['uid']==10165]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbcfd38",
   "metadata": {},
   "source": [
    "## Attempt 2: Item-based collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1051aabd",
   "metadata": {},
   "source": [
    "### Create and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bf9cc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x15e46bbd0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REPEATED, maybe should remove\n",
    "# # Define rating scale\n",
    "# reader = Reader(rating_scale=(1, 10))\n",
    "\n",
    "# # Load data into Surprise dataset format\n",
    "# data = Dataset.load_from_df(merged_data[['profile', 'uid', 'score']], reader)\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build item-based collaborative filtering model\n",
    "sim_options = {'name': 'cosine', 'user_based': False}  # Set user_based to False for item-based\n",
    "model_2 = KNNBasic(sim_options=sim_options)\n",
    "model_2.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b3cec",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b02bcae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.9756\n",
      "MAE:  1.4661\n",
      "RMSE: 1.9756028737805376\n",
      "MAE: 1.466091411640768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_2.test(testset)\n",
    "\n",
    "# Calculate RMSE and MAE\n",
    "rmse = rmse(predictions)\n",
    "mae = mae(predictions)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81111138",
   "metadata": {},
   "source": [
    "### Testing by inputing your liked animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e818a88d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 11061 is out of bounds for axis 0 with size 577",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m recommended \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m anime_id \u001b[38;5;129;01min\u001b[39;00m liked_anime_ids:\n\u001b[0;32m---> 11\u001b[0m     recommended\u001b[38;5;241m.\u001b[39mextend(recommend_anime(anime_id, model_2))  \u001b[38;5;66;03m# Use extend() instead of add()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Count the occurrences of each recommended anime ID\u001b[39;00m\n\u001b[1;32m     14\u001b[0m anime_counts \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m, in \u001b[0;36mrecommend_anime\u001b[0;34m(anime_uid, model, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommend_anime\u001b[39m(anime_uid, model, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Get the k nearest neighbors (items) for the given anime UID\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     anime_neighbors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_neighbors(anime_uid, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m anime_neighbors\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/surprise/prediction_algorithms/algo_base.py:288\u001b[0m, in \u001b[0;36mAlgoBase.get_neighbors\u001b[0;34m(self, iid, k)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     all_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset\u001b[38;5;241m.\u001b[39mall_items\n\u001b[0;32m--> 288\u001b[0m others \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim[iid, x]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_instances() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m!=\u001b[39m iid]\n\u001b[1;32m    289\u001b[0m others \u001b[38;5;241m=\u001b[39m heapq\u001b[38;5;241m.\u001b[39mnlargest(k, others, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m tple: tple[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    290\u001b[0m k_nearest_neighbors \u001b[38;5;241m=\u001b[39m [j \u001b[38;5;28;01mfor\u001b[39;00m (j, _) \u001b[38;5;129;01min\u001b[39;00m others]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/surprise/prediction_algorithms/algo_base.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     all_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset\u001b[38;5;241m.\u001b[39mall_items\n\u001b[0;32m--> 288\u001b[0m others \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim[iid, x]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_instances() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m!=\u001b[39m iid]\n\u001b[1;32m    289\u001b[0m others \u001b[38;5;241m=\u001b[39m heapq\u001b[38;5;241m.\u001b[39mnlargest(k, others, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m tple: tple[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    290\u001b[0m k_nearest_neighbors \u001b[38;5;241m=\u001b[39m [j \u001b[38;5;28;01mfor\u001b[39;00m (j, _) \u001b[38;5;129;01min\u001b[39;00m others]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11061 is out of bounds for axis 0 with size 577"
     ]
    }
   ],
   "source": [
    "# hunter x hunter, tokyo ghoul, Ansatsu_Kyoushitsu, Kono_Subarashii_Sekai_ni_Shukufuku_wo, Kaguya-sama, Nichijou\n",
    "liked_anime_ids = [11061, 22319, 40748, 24833, 30831, 37999, 10165]\n",
    "\n",
    "def recommend_anime(anime_uid, model, k=10):\n",
    "    # Get the k nearest neighbors (items) for the given anime UID\n",
    "    anime_neighbors = model.get_neighbors(anime_uid, k=k)\n",
    "    return anime_neighbors\n",
    "\n",
    "recommended = []\n",
    "for anime_id in liked_anime_ids:\n",
    "    recommended.extend(recommend_anime(anime_id, model_2))\n",
    "\n",
    "# Count the occurrences of each recommended anime ID\n",
    "anime_counts = {}\n",
    "for anime_id in recommended:\n",
    "    anime_counts[anime_id] = anime_counts.get(anime_id, 0) + 1\n",
    "\n",
    "# Get the top 10 most frequent anime IDs\n",
    "top_10_anime = sorted(anime_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 Most Recommended Anime:\")\n",
    "print(top_10_anime)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf54555e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Anime Titles:\n",
      "Cowboy Bebop\n",
      "Cowboy Bebop: Tengoku no Tobira\n",
      "Trigun\n",
      "Witch Hunter Robin\n",
      "Bouken Ou Beet\n",
      "Eyeshield 21\n",
      "Hachimitsu to Clover\n",
      "Hungry Heart: Wild Striker\n",
      "Initial D Fourth Stage\n",
      "Monster\n"
     ]
    }
   ],
   "source": [
    "###WASTE\n",
    "# hunter x hunter, tokyo ghoul, Ansatsu_Kyoushitsu, Kono_Subarashii_Sekai_ni_Shukufuku_wo\n",
    "#Kaguya-sama, Nichijou\n",
    "\n",
    "anime_ids = anime_data.uid.unique()\n",
    "liked_anime_ids = [11061, 22319, 40748, 24833, 30831, 37999, 10165]\n",
    "\n",
    "# Predict ratings for animes i have not watched\n",
    "predicted_ratings = {}\n",
    "for anime_id in anime_ids:\n",
    "    if anime_id not in liked_anime_ids:\n",
    "        predicted_rating = model.predict(uid=None, iid=anime_id).est\n",
    "        predicted_ratings[anime_id] = predicted_rating\n",
    "\n",
    "# Sort predicted ratings and recommend top N items\n",
    "top_n = 10\n",
    "recommended_anime_ids = sorted(predicted_ratings, key=predicted_ratings.get, reverse=True)[:top_n]\n",
    "\n",
    "# Print recommended anime titles\n",
    "recommended_anime_titles = anime_data[anime_data['uid'].isin(recommended_anime_ids)]['title'].values\n",
    "print(\"Recommended Anime Titles:\")\n",
    "for title in recommended_anime_titles:\n",
    "    print(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bfda96",
   "metadata": {},
   "source": [
    "### SVD Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "552ee9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.8287\n",
      "RMSE: 1.828723672662592\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Define rating scale\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "\n",
    "# Load data into Surprise dataset format\n",
    "data = Dataset.load_from_df(merged_data[['profile', 'uid', 'score']], reader)\n",
    "\n",
    "# Split data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build SVD collaborative filtering model\n",
    "model = SVD()\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = rmse(predictions)\n",
    "print(\"RMSE:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f60b67d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m predicted_matrix \u001b[38;5;241m=\u001b[39m df_predictions\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miid\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create heatmap of predicted ratings\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     11\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(predicted_matrix, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m, cbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItem ID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "df_predictions = pd.DataFrame(predictions, columns=['uid', 'iid', 'actual', 'predicted', 'details'])\n",
    "\n",
    "# Pivot predictions DataFrame to create a user-item matrix\n",
    "predicted_matrix = df_predictions.pivot(index='uid', columns='iid', values='predicted')\n",
    "\n",
    "# Create heatmap of predicted ratings\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(predicted_matrix, cmap='viridis', cbar=True, linewidths=0.5)\n",
    "plt.xlabel('Item ID')\n",
    "plt.ylabel('User ID')\n",
    "plt.title('Predicted Ratings Heatmap')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972b9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
